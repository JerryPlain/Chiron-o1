# Chiron-o1 两小时速读清单

目标：在 120 分钟内建立可操作的项目心智模型，能回答“这项目怎么跑、核心算法怎么工作、我该从哪改起”。

阅读顺序：`README.md` -> `src/run.py` -> `src/mics.py` -> `src/utils.py` -> `src/model.py` -> `src/prompt.py` -> `src/step.py` -> `eval/eval.py`

## 使用方式

- 每个阶段只做三件事：看指定文件、回答关键问题、写 2-4 行结论。
- 不追求一次看懂全部细节，先跑通主链路，再补局部实现。
- 看到不确定点，先记在“待验证问题”，最后统一验证。

## 0-10 分钟：项目定位与运行边界

看：`README.md`

你要回答：
- 这个仓库主要解决什么问题？关键词是什么（MICS、mentor/intern、medical reasoning）？
- 仓库提供的是哪三类能力：数据构造、推理、评测？
- 最低可运行配置是什么（至少几个 mentor、几个 intern、GPU/接口依赖）？

输出记录模板：
- 项目一句话：
- 最小运行依赖：
- 我今天关注范围（先不看哪些内容）：

## 10-25 分钟：入口和执行骨架

看：`src/run.py`

你要回答：
- 入口函数是什么？主流程有哪些阶段（读数据、初始化模型、逐样本搜索、写结果）？
- CLI 参数分几类（路径、模型、API、搜索超参）？
- 程序如何处理失败（`_failed.jsonl`、异常兜底）？

重点盯函数：
- `mics_start(args)`
- 参数校验段（`mentor_models` / `intern_models`）

输出记录模板：
- 主流程 5 步：
- 输入输出文件契约：
- 参数里最关键的 5 个：

## 25-55 分钟：核心算法主循环（最关键）

看：`src/mics.py`

你要回答：
- 一层搜索内发生什么：mentor 生成 -> intern 评估 -> 选下一步？
- 何时提前成功（score=1.0）？何时提前失败（all zero）？
- `reasoning_chains`、`mentors_scores`、`previous_mentors` 各自用途是什么？
- 模型路由 `_call_model_forward` 是按什么规则分发的？

重点盯函数：
- `search(...)`
- `_generate_next_step_with_mentor(...)`
- `_evaluate_step_with_interns(...)`
- `_call_model_forward(...)`

输出记录模板：
- 单样本时序图（文字版 6-8 步）：
- 打分公式：
- 提前终止条件：

## 55-75 分钟：工具函数与真实数据契约

看：`src/utils.py`

你要回答：
- 图片路径如何解析（`locate_img`）？失败会在哪报错？
- evaluator 判定逻辑是什么（`get_correctness`）？是否有误判风险？
- 同分时如何选 step/mentor（`select_next_step`、`select_best_mentor`）？
- 文本处理函数（`extract_first_step` 等）影响了哪些主流程行为？

重点盯函数：
- `locate_img(...)`
- `get_correctness(...)`
- `select_next_step(...)`
- `select_best_mentor(...)`

输出记录模板：
- 数据字段最小集合：
- 3 个高风险点：
- 若我要先改一处，会改哪里：

## 75-90 分钟：模型加载与资源假设

看：`src/model.py`

你要回答：
- intern 模型各自如何加载，最终 `model_set` 长什么样？
- `split_model` 对硬件做了什么假设（多卡 CUDA）？
- 如果我只想跑最小实验，哪些模型和参数可以先关掉？

重点盯函数：
- `init_model(args)`
- `split_model(model_path)`

输出记录模板：
- `model_set` 结构草图：
- 硬件/环境前提：
- 最小可跑组合：

## 90-105 分钟：Prompt 约束与树结构

看：`src/prompt.py` + `src/step.py`

你要回答：
- 三类 prompt 分别负责什么（生成、评估、裁判）？
- 输出格式约束中，哪一条最脆弱（比如依赖 `### The final answer is:`）？
- `Step` 如何保存路径上下文（`parent`、`prefix_steps`、`text`）？

重点盯内容：
- `REASONING_PROMPT`
- `EVALUATE_PROMPT`
- `JUDGE_PROMPT`
- `Step.add_child_step(...)`

输出记录模板：
- Prompt 依赖项清单：
- Step 数据结构一句话定义：
- 1 个你认为应该加强的鲁棒性点：

## 105-120 分钟：评测口径与收尾总结

看：`eval/eval.py`

你要回答：
- 评测输入输出是什么？
- 评测指标有哪些（闭集正确性 + 语义分）？
- 评测脚本和 MICS 主流程的关系是什么（在线搜索 vs 离线评测）？

输出记录模板：
- 我理解的评测流程 4 步：
- 我会优先看的 2 个指标：
- 与主流程最容易混淆的一点：

## 最后 10 分钟复盘（必须做）

用下面模板写出你自己的“项目心智模型”：

1. 项目一句话
- 

2. 端到端链路（输入 -> 处理 -> 输出）
- 

3. 核心算法一句话
- 

4. 三个关键可改点（按优先级）
- 
- 
- 

5. 我下一步要做的最小实验
- 命令：
- 预期输出：
- 失败时先查哪里：

## 两小时内不要做的事

- 不要先优化代码风格。
- 不要先改训练脚本。
- 不要先追论文细节到实现外。
- 不要一次性接入新模型。

## 建议的阅读纪律

- 每 15 分钟写一次结论，不要只滚动看代码。
- 遇到不懂的函数，先看它在上游/下游的调用位置。
- 优先回答“这个函数对主链路有什么影响”，其次才是实现细节。
